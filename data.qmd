---
title: Data
description: We describe the sources of our data and the cleaning process. The data was sourced from the U.S. Department of Education (https://collegescorecard.ed.gov/data/).
toc: true
draft: false
---

![](images/Department_of_education.png)


This comes from the file `data.qmd`.

## Overview of the College Scorecard
The data is sourced from the U.S. Department of Education. It includes institution-level data files from the 1996-97 academic year through the 2022-23 academic year, containing aggregate information for each institution. 

*The dataset was last updated on October 10, 2024 and can be accessed using the following [link](https://collegescorecard.ed.gov/data).*

## Background and Purpose

The College Scorecard was created during the Obama Administration to provide transparent, accessible information to help students and families make informed decisions about higher education. It compiles data on metrics such as net price, graduation rates, student diversity, median graduate debt, salary data, and student loan repayment information.

The goal of this dataset was to create a federal tool that offers comprehensive institutional profiles, synthesizing complex and multi-dimensional aspects of higher education into understandable and standardized metrics.

*Article found [here](https://obamawhitehouse.archives.gov/blog/2015/09/12/weekly-address-new-college-scorecard).*

![](images/Announcement_Article.png)

## Analysis on Flaws and Limitations

Although serving as a comprehensive and centralized federal postsecondary ratings system, the College Scorecard still contains flaws that undermine its overall effectiveness:

- **Lack of Qualitative Information:** The Scorecard focuses primarily on quantitative variables, particularly monetary measures of value. This narrow focus and lack of qualitative information can make the college search process confusing for students and their families.

- **Past Inaccurate Data:** In 2017, the Scorecard faced criticism for publishing inaccurate loan repayment rates for most colleges. The Department fixed these mistakes, attributing these inaccuracies due to a “coding blunder.” 

- **Emphasis on Financial Metrics:** By reducing the measure of an institution’s worth to numerical factors, the Scorecard does not fully encapsulate an institution’s mission or character. This emphasis on financial metrics strips away the nuances of an institution and can lead to an inaccurate portrayal of the college experience.


## Data Files and Variable Overview
Institution-level data files derived from the College Scorecard were the focal point of data analysis, including information on institutional characteristics, enrollment, costs, and student outcomes.

- **Institution Basics:** Institution, State, Historically_Black
- **Admissions and Testing:** Acceptance_rate, Average_SAT
- **Average cost and net price contigent on financial bracket:** Average_Cost_Of_Attendance, Avg_Net_Price_Pub_0_To_30000_Income, Avg_Net_Price_Pub_30000_To_48000_Income, Avg_Net_Price_Pub_48001_To_75000_Income, Avg_Net_Price_Pub_75001_To_110000_Income, Avg_Net_Price_Pub_110001_To_Inf_Income
- **Racial composition:** Perc_Undergrad_White, Perc_Undergrad_Black, Perc_Undergrad_Hispanic, Perc_Undergrad_Asian, Perc_Undergrad_Am_Indian_Alask_Native, Perc_Native_Hawaiin_Pac_Islander, Perc_Two_Or_More_Races
- **Financial aid and loan:** Perc_Receiving_Loan, Med_Loan_Debt
- **Completition rate:** White_Compl_Rate, Black_Compl_Rate, Hisp_Compl_Rate, Asian_Compl_Rate, Asian_Am_Pac_Islander_Compl_Rate, Native_Am_Pac_Islander_Compl_Rate, Two_Or_More_Races_Compl_Rate
- **Earnings after college:** Med_earnings

## Data Cleaning Process
*Cleaned College Scorecard data can be found [here](/scripts/load_and_clean_data.R).*
```{r}
source("scripts/load_and_clean_data.R")
```

The original loaded education_cohort_data was *6484 rows x 3305 columns*. The cleaned dataset is *530 rows x 43 columns*. 

The **College Scorecard Data Dictionary** was used to parse through abbreviated terminology and retain all relevant data values. Columns were renamed to improve readability. Data entries missing key values, including acceptance rate, tuition, racial demographics, median loan amount, percentage receiving loan, and completion rates, were removed.

*Technical documentation on the data can be found [here](https://collegescorecard.ed.gov/assets/InstitutionDataDocumentation.pdf).*

```{r eval=FALSE}
# Condensed description of data cleaning process

# Loaded in the data
education_cohort_data <- read_csv(here::here("dataset-ignore","Most-Recent-Cohorts-Institution.csv"))

# Documented relevant data columns
table_cols <- c(
  # Institution and demographics on student population
  "INSTNM", "STABBR", ...
  
  # Repeated this process for all relevant categories
  
# Change data names
education_cohort_data <- education_cohort_data |>
  rename(
    Institution = INSTNM,
    State = STABBR, ...
    
    # Repeated this process for all relevant categories
    
# Removed rows that listed NA for key numerical values
education_cohort_data <- education_cohort_data |>
  filter_at(vars(one_of(cols_check_for_na)), all_vars(!is.na(.)))
```



Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.


Initially, you will study _one dataset_ but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable.
Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.


## What makes a good data set?

* Data you are interested in and care about.
* Data where there are a lot of potential questions that you can explore.
* A data set that isn't completely cleaned already.
* Multiple sources for data that you can combine.
* Some type of time and/or location component.


## Where to keep data?


Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data


For small datasets (<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  "scripts/load_and_clean_data.R",
  echo = TRUE # Use echo=FALSE or omit it to avoid code output  
)
```
You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R). 

----

## Rubric: On this page

You will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Rename variables and recode factors to make data more clear.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.